{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67900b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convet labeled data in JARVIS to the Lightning Pose format\n",
    "# Lightening Pose provides a function to convert DLC labeled data to LP labeled data\n",
    "# Reference\n",
    "# https://github.com/danbider/lightning-pose/blob/main/scripts/converters/dlc2lp.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c82f8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5141f6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "jarvis_dir = r'E:\\Hand_tracking\\Datasets\\annotation\\6cam_dataset_231216' # path to the labeled dataset \n",
    "lp_dir = r'E:\\Hand_tracking\\LP_projects\\LP_240120' # path to the lp project\n",
    "\n",
    "# find all labeled data in JARVIS project\n",
    "prefix_year = '2023' # Trial names starts with XXXX year. \n",
    "dirs = [filename for filename in os.listdir(jarvis_dir) if filename.startswith(prefix_year)]\n",
    "dirs.sort()\n",
    "dfs = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f31f0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in dirs:\n",
    "    cameras= os.listdir(os.path.join(jarvis_dir, d))\n",
    "    for c in cameras:\n",
    "        csv_file = glob.glob(os.path.join(jarvis_dir, d, c, \"annotations.csv\"))[0]\n",
    "        \n",
    "        df1 = pd.read_csv(csv_file, on_bad_lines='skip', header = None, index_col=0) \n",
    "        df2 = pd.read_csv(csv_file, skiprows=4, header = None, index_col=0) \n",
    "        last_column = df2.shape[1]\n",
    "        df2 = df2.drop(columns=[last_column],axis='columns')\n",
    "\n",
    "        # Remove entities row\n",
    "        isNotEntities = [x != 'entities' for x in df1.index.values]\n",
    "        df1 = df1.iloc[isNotEntities]\n",
    "        # Find coords row and remove state columns\n",
    "        isCoords = [x == 'coords' for x in df1.index.values]\n",
    "        isXY = [s != 'state' for s in df1.iloc[isCoords].values]\n",
    "        df1 = df1.iloc[:,isXY[0]]\n",
    "        df2 = df2.iloc[:, isXY[0]]\n",
    "        # Replace image file name with its file path\n",
    "        vid = d + '_' + c\n",
    "        imgs = list(df2.index.values)\n",
    "        # Change .jpg to .png (JARVIS- .jpg, LP/DLC- .png)\n",
    "        im_idx = [i[6:len(i)-4] for i in imgs]\n",
    "        imgs_new =['img' + format(int(i), '04d') + \".png\" for i in im_idx]\n",
    "        new_col = [f\"labeled-data/{vid}/{i}\" for i in imgs_new]\n",
    "        df2.index = new_col\n",
    "\n",
    "        df_tmp = pd.concat([df1,df2])\n",
    "        \n",
    "        df_tmp.to_csv(os.path.join(jarvis_dir,d,c, \"CollectedData.csv\"), header = False)\n",
    "        df = pd.read_csv(os.path.join(jarvis_dir,d,c, \"CollectedData.csv\"), header = [0,1,2], index_col=0)\n",
    "        \n",
    "        dfs.append(df)\n",
    "df_all = pd.concat(dfs)\n",
    "\n",
    "os.makedirs(lp_dir, exist_ok=True)\n",
    "\n",
    "# save concatenated labels\n",
    "df_all.to_csv(os.path.join(lp_dir, \"CollectedData.csv\"))\n",
    "            \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4da66a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "src_vid_dir = r'E:\\Hand_tracking\\Recordings\\Videos'\n",
    "os.makedirs(os.path.join(lp_dir,'videos'), exist_ok=True)\n",
    "for d in dirs:\n",
    "    cameras= os.listdir(os.path.join(jarvis_dir,d))\n",
    "    for c in cameras:\n",
    "        vid = d + '_' + c # new video name\n",
    "        os.makedirs(os.path.join(lp_dir,\"labeled-data\",vid), exist_ok=True)\n",
    "        # Convert .jpg to .png and copy frames over\n",
    "        imgs = [im for im in os.listdir(os.path.join(jarvis_dir,d,c)) if im.endswith('.jpg')]\n",
    "        for i in imgs:\n",
    "            im = Image.open(os.path.join(jarvis_dir,d,c,i))\n",
    "            im_idx = i[6:len(i)-4] \n",
    "            new_name ='img' + format(int(im_idx), '04d') + \".png\" \n",
    "            im.save(os.path.join(lp_dir,\"labeled-data\", vid, new_name))\n",
    "        \n",
    "        # copy videos over\n",
    "        session = d[0:10]\n",
    "        src = os.path.join(src_vid_dir, session, d, c+'.mp4')\n",
    "        dst = os.path.join(lp_dir, \"videos\", vid+'.mp4')\n",
    "        shutil.copy(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "435b0b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "for im in df_all.index:\n",
    "    assert os.path.exists(os.path.join(lp_dir, im))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc9a133",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2d1915",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16eb003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--dlc_dir\", type=str)\n",
    "parser.add_argument(\"--lp_dir\", type=str)\n",
    "args = parser.parse_args()\n",
    "dlc_dir = args.dlc_dir\n",
    "lp_dir = args.lp_dir\n",
    "\n",
    "print(f\"Converting DLC project located at {dlc_dir} to LP project located at {lp_dir}\")\n",
    "\n",
    "# check provided DLC path exists\n",
    "if not os.path.exists(dlc_dir):\n",
    "    raise NotADirectoryError(f\"did not find the directory {dlc_dir}\")\n",
    "\n",
    "# check paths are not the same\n",
    "if dlc_dir == lp_dir:\n",
    "    raise NameError(f\"dlc_dir and lp_dir cannot be the same\")\n",
    "\n",
    "# find all labeled data in DLC project\n",
    "dirs = os.listdir(os.path.join(dlc_dir, \"labeled-data\"))\n",
    "dirs.sort()\n",
    "dfs = []\n",
    "\n",
    "for d in dirs:\n",
    "    print(d)\n",
    "    try:\n",
    "        csv_file = glob.glob(os.path.join(dlc_dir, \"labeled-data\", d, \"CollectedData*.csv\"))[0]\n",
    "        df_tmp = pd.read_csv(csv_file, header=[0, 1, 2], index_col=0)\n",
    "        if len(df_tmp.index.unique()) != df_tmp.shape[0]:\n",
    "            # new DLC labeling scheme that splits video/image in different cells\n",
    "            vids = df_tmp.loc[\n",
    "                   :, (\"Unnamed: 1_level_0\", \"Unnamed: 1_level_1\", \"Unnamed: 1_level_2\")]\n",
    "            imgs = df_tmp.loc[\n",
    "                   :, (\"Unnamed: 2_level_0\", \"Unnamed: 2_level_1\", \"Unnamed: 2_level_2\")]\n",
    "            new_col = [f\"labeled-data/{v}/{i}\" for v, i in zip(vids, imgs)]\n",
    "            df_tmp1 = df_tmp.drop(\n",
    "                (\"Unnamed: 1_level_0\", \"Unnamed: 1_level_1\", \"Unnamed: 1_level_2\"), axis=1,\n",
    "            )\n",
    "            df_tmp2 = df_tmp1.drop(\n",
    "                (\"Unnamed: 2_level_0\", \"Unnamed: 2_level_1\", \"Unnamed: 2_level_2\"), axis=1,\n",
    "            )\n",
    "            df_tmp2.index = new_col\n",
    "            df_tmp = df_tmp2\n",
    "    except IndexError:\n",
    "        try:\n",
    "            h5_file = glob.glob(os.path.join(dlc_dir, \"labeled-data\", d, \"CollectedData*.h5\"))[0]\n",
    "            df_tmp = pd.read_hdf(h5_file)\n",
    "            if type(df_tmp.index) == pd.core.indexes.multi.MultiIndex:\n",
    "                # new DLC labeling scheme that splits video/image in different cells\n",
    "                imgs = [i[2] for i in df_tmp.index]\n",
    "                vids = [df_tmp.index[0][1] for _ in imgs]\n",
    "                new_col = [f\"labeled-data/{v}/{i}\" for v, i in zip(vids, imgs)]\n",
    "                df_tmp1 = df_tmp.reset_index().drop(\n",
    "                    columns=\"level_0\").drop(columns=\"level_1\").drop(columns=\"level_2\")\n",
    "                df_tmp1.index = new_col\n",
    "                df_tmp = df_tmp1\n",
    "        except IndexError:\n",
    "            print(f\"Could not find labels for {d}; skipping\")\n",
    "    dfs.append(df_tmp)\n",
    "df_all = pd.concat(dfs)\n",
    "\n",
    "os.makedirs(lp_dir, exist_ok=True)\n",
    "\n",
    "# save concatenated labels\n",
    "df_all.to_csv(os.path.join(lp_dir, \"CollectedData.csv\"))\n",
    "\n",
    "# copy frames over\n",
    "src = os.path.join(dlc_dir, \"labeled-data\")\n",
    "dst = os.path.join(lp_dir, \"labeled-data\")\n",
    "shutil.copytree(src, dst)\n",
    "\n",
    "# copy videos over\n",
    "src = os.path.join(dlc_dir, \"videos\")\n",
    "dst = os.path.join(lp_dir, \"videos\")\n",
    "if os.path.exists(src):\n",
    "    print(\"copying video files\")\n",
    "    shutil.copytree(src, dst)\n",
    "else:\n",
    "    print(\"DLC video directory does not exist; creating empty video directory\")\n",
    "    os.makedirs(dst, exist_ok=True)\n",
    "\n",
    "# check\n",
    "for im in df_all.index:\n",
    "    assert os.path.exists(os.path.join(lp_dir, im))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jarvis",
   "language": "python",
   "name": "jarvis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
